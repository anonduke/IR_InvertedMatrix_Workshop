{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## ðŸ” Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed codeâ€”just like in the real world.*\n",
    "\n",
    "# Team Members\n",
    "## Fasalu Rahman Kottaparambu 8991782\n",
    "## Srinu Babu Rai 8994032"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data â€” such as AI agents.\n",
    "\n",
    "### ðŸ‘¥ Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## ðŸ”§ Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ðŸ§  Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## ðŸ§© Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* â€“ Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* â€“ Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* â€“ Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* â€“ Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## ðŸ’» Submission Checklist\n",
    "- âœ… `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- âœ… `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- âœ… GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## ðŸ“„ Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0258aadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fasal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 documents.\n"
     ]
    }
   ],
   "source": [
    "# Example: Load text files from a folder\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Replace 'sample_docs/' with your actual folder\n",
    "documents = load_documents(\"./documents\")\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'and', 'big', 'data', 'analytics', 'introduction', 'the', 'combination', 'of', 'artificial', 'intelligence', 'and', 'big', 'data', 'analytics', 'enables', 'organizations', 'to', 'extract', 'valuable']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    tokens = tokens = re.findall(r'\\b[a-zA-Z0-9]+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Test on one document\n",
    "tokens = tokenize(documents[0])\n",
    "print(tokens[:20])  # Preview first 20 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## ðŸ” Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'big', 'data', 'analyt', 'introduct', 'combin', 'artifici', 'intellig', 'big', 'data', 'analyt', 'enabl', 'organ', 'extract', 'valuabl', 'insight', 'massiv', 'complex', 'dataset', 'ai']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fasal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def normalize_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "\n",
    "# Example: normalize one document\n",
    "norm_tokens = normalize_tokens(tokens)\n",
    "print(norm_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## ðŸ” Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Build the inverted index using a dictionary.\n",
    "- Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai: {0: [0, 19, 158, 189], 1: [0, 17, 94, 130, 148, 154], 2: [0, 38, 40, 73, 90, 180], 3: [0, 21, 73, 171], 4: [0, 40, 169], 5: [0, 20, 35, 162, 172], 6: [0, 15, 189], 7: [0, 16, 31, 54, 115, 141, 151, 156, 161], 8: [0, 18, 32, 62, 180, 187], 9: [0, 16, 30, 84, 109, 144], 10: [7, 24, 45, 48, 63, 113, 181, 184, 191, 204, 206, 282, 317, 325, 372, 389, 412, 420, 425, 443, 457, 470, 478, 486, 509, 542, 566, 581, 600, 604, 625, 628, 649, 686], 13: [1, 12, 16, 36, 56, 162, 167, 176], 14: [1, 19, 28, 45, 50, 55, 60, 66, 78, 85, 113, 123, 159, 186, 188, 201], 16: [25], 18: [2, 59, 115, 124, 145, 158]}\n",
      "big: {0: [1, 8, 36, 190], 8: [117]}\n",
      "data: {0: [2, 9, 23, 29, 37, 43, 54, 72, 114, 115, 120, 140, 164, 178, 183, 191], 1: [74, 132], 2: [36, 102, 116, 124, 150, 166, 171], 3: [88], 4: [27, 33, 38, 92, 143, 151, 164], 5: [25, 67, 96, 145, 154], 6: [85], 7: [91, 128, 137], 8: [118, 132, 157, 162], 9: [115], 10: [66, 133, 290, 510, 525], 11: [160], 12: [26, 45, 90, 122, 128, 148, 190, 230], 13: [107], 14: [48], 15: [15, 37, 50, 73, 78], 16: [14, 53, 72, 124, 177, 199, 287, 298], 17: [108, 117, 136], 18: [24, 88]}\n",
      "analyt: {0: [3, 10, 51, 93, 199], 1: [59, 169], 2: [14], 5: [107], 7: [68], 8: [119], 9: [42], 14: [112]}\n",
      "introduct: {0: [4], 1: [2], 2: [3], 3: [2], 4: [3], 5: [3], 6: [3], 7: [2], 8: [2], 9: [2], 10: [4], 11: [4], 12: [3], 13: [2], 14: [3], 15: [3], 16: [3], 17: [4], 18: [3], 19: [3]}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_positional_inverted_index(documents):\n",
    "    index = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, text in enumerate(documents):\n",
    "        tokens = normalize_tokens(tokenize(text))\n",
    "        for position, token in enumerate(tokens):\n",
    "            index[token][doc_id].append(position)\n",
    "    return index\n",
    "\n",
    "positional_inverted_index = build_positional_inverted_index(documents)\n",
    "\n",
    "# Preview first 5 terms\n",
    "for term in list(positional_inverted_index.keys())[:5]:\n",
    "    print(f\"{term}: {dict(positional_inverted_index[term])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## ðŸ§ª Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—£ Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### ðŸ”§ Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with exact phrase 'machine learning': [0, 1, 2, 3, 4, 5, 7, 8, 12, 15, 16, 17, 18, 19]\n",
      "Documents with exact phrase 'deep learning': [6, 10, 12, 13, 15, 16]\n",
      "Documents with exact phrase 'natural language': [0, 1, 3, 7, 8, 17]\n",
      "Documents with exact phrase 'data': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "Documents with exact phrase 'big data': [0, 8]\n"
     ]
    }
   ],
   "source": [
    "# Phrase Query Implementation using Positional Inverted Index\n",
    "\n",
    "def phrase_query(phrase, index):\n",
    "    terms = [stemmer.stem(t) for t in tokenize(phrase) if t not in stop_words]\n",
    "    if not terms:\n",
    "        return []\n",
    "    potential_docs = set(index.get(terms[0], {}).keys())\n",
    "    for term in terms[1:]:\n",
    "        potential_docs &= set(index.get(term, {}).keys())\n",
    "    result_docs = []\n",
    "    for doc_id in potential_docs:\n",
    "        positions = [index[term][doc_id] for term in terms]\n",
    "        for pos in positions[0]:\n",
    "            if all((pos + offset) in positions[offset] for offset in range(len(terms))):\n",
    "                result_docs.append(doc_id)\n",
    "                break\n",
    "    return result_docs\n",
    "\n",
    "# Example phrase queries\n",
    "query1 = \"machine learning\"\n",
    "query2 = \"deep learning\"\n",
    "query3 = \"natural language\"\n",
    "query4 = \"data\"\n",
    "query5 = \"big data\"\n",
    "\n",
    "\n",
    "print(f\"Documents with exact phrase '{query1}':\", phrase_query(query1, positional_inverted_index))\n",
    "print(f\"Documents with exact phrase '{query2}':\", phrase_query(query2, positional_inverted_index))\n",
    "print(f\"Documents with exact phrase '{query3}':\", phrase_query(query3, positional_inverted_index))\n",
    "print(f\"Documents with exact phrase '{query4}':\", phrase_query(query4, positional_inverted_index))\n",
    "print(f\"Documents with exact phrase '{query5}':\", phrase_query(query5, positional_inverted_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce119b5",
   "metadata": {},
   "source": [
    "# Taking points\n",
    "1. Learned how to preprocess documents by tokenizing, removing stopwords, and applying stemming to prepare data for indexing.\n",
    "2. Implemented Boolean queries using AND, OR, and NOT operations to find documents containing specific terms quickly.\n",
    "3. Created and tested phrase queries that allow us to find documents containing exact phrases or word using our positional inverted index.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
